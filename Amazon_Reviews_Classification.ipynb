{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon-Reviews-Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcOe26IW_gs-",
        "colab_type": "text"
      },
      "source": [
        "Text Classification is an example of supervised machine learning task. We have a labelled dataset containing text documents and their corresponding labels. An end-to-end text classification pipeline is composed of three main components:\n",
        "\n",
        "1. Dataset Preparation: The first step is the dataset preparation which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.\n",
        "2. Feature Engineering: In this step the raw dataset is transformed into features which can be then fed into a machine learning model. \n",
        "3. Model Training: The final step is the model building step in which a machine learning model is trained on a labelled dataset. We would be using the Multinomial Naive Bayes, SVM, Logistic Regression, Xtereme Gradient Boosting, NN algorithms to categorize the Amazon reviews.\n",
        "\n",
        "Understanding LSTM's\n",
        "\n",
        "*  https://colah.github.io/\n",
        "*  https://www.youtube.com/watch?v=8HyCNIVRbSU\n",
        "\n",
        "Word Embedding Explained\n",
        "\n",
        "*  http://jalammar.github.io/illustrated-word2vec/\n",
        "*  https://ruder.io/word-embeddings-1/index.html\n",
        "*  http://mccormickml.com/tutorials/\n",
        "\n",
        "Word Embedding - Code Implementation in Keras\n",
        "\n",
        "*   https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "*   https://keras.io/layers/embeddings/ - Refer the example. Arguments passed to ***Embedding()*** such as ***input_dim***: Size of the vocabulary and ***output_dim***: Dimension of the dense embedding. The Input to a Embedding layer is a 2D tensor with shape: (batch_size, sequence_length). The Output from a Embedding layer is a 3D tensor with shape: (batch_size, sequence_length, output_dim).\n",
        "\n",
        "Implementation of LSTM in Keras\n",
        "\n",
        "*  https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47\n",
        "\n",
        "*  https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/\n",
        "\n",
        "Keras Tokenizer method\n",
        "\n",
        "*  https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wxPCH0_i1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install textblob\n",
        "# !pip install gensim\n",
        "import numpy as np\n",
        "import pandas, xgboost, textblob, string\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import optimizers\n",
        "from keras import layers, models, optimizers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjiVSoj-0-7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ie82gWzZ-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0uw5-ehzb-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded['corpus']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-rO5KuKzgr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "https://stackoverflow.com/questions/45482272/typeerror-a-bytes-like-object-is-required-not-str-python-2-to-3\n",
        "https://stackoverflow.com/questions/48277354/typeerror-a-bytes-like-object-is-required-not-str-how-can-i-fix-this\n",
        "The gist is that bytes are, well, bytes (groups of 8 bits without any further meaning attached), whereas characters are the things that make up strings of text. \n",
        "Encoding turns characters i.e str into bytes, and decoding turns bytes back into characters.\n",
        "The encode() method encodes the string, using the specified encoding. If no encoding is specified, UTF-8 will be used.\n",
        "So basically string data is converted to bytes data : data_bytes = data_string.decode(\"utf-8\")\n",
        "It's because type(uploaded['corpus']) returns bytes, just decode it using utf-8.\n",
        "\n",
        "RUN THE BELOW CODE IN A SEPERATE CELL\n",
        "\n",
        "# initializing string  \n",
        "str = \"geeksforgeeks\"\n",
        "  \n",
        "# encoding string  \n",
        "str_enc = str.encode() \n",
        "  \n",
        "# printing the encoded string \n",
        "print (\"The encoded string in base64 format is : \")  \n",
        "print(str_enc)\n",
        "  \n",
        "# printing the original decoded string  \n",
        "print (\"The decoded string is : \")\n",
        "print(str_enc.decode())\n",
        "\n",
        "'''\n",
        "print(type(uploaded['corpus']))\n",
        "print(type(uploaded['corpus'].decode('utf-8')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClDk8YQcwFhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6a2e196e-595f-4c49-e4ca-f0763d59ae03"
      },
      "source": [
        "'''\n",
        "\\n is what you do when you go to next line i.e ‘Enter Key’ and for tabs we use \\t\n",
        "\n",
        "RUN THE BELOW CODE IN A SEPERATE CELL\n",
        "\n",
        "text = 'The oceans are warming faster than previously estimated, setting a new temperature record in 2018 in a trend that is damaging marine life, scientists said on Thursday.\\nNew measurements, aided by an international network of 3,900 floats deployed in the oceans since 2000, showed more warming since 1971 than calculated by the latest UN assessment of climate change in 2013, they said.\\nAnd “observational records of ocean heat content show that ocean warming is accelerating,” the authors in China and the US wrote in the journal Science of ocean waters down to 2,000 metres (6,600 ft).'\n",
        "print(text)\n",
        "print('--')\n",
        "print(text.split('\\n'))\n",
        "\n",
        "RUN THE BELOW CODE IN A SEPERATE CELL\n",
        "\n",
        "my_list = ['apple', 'banana', 'grapes', 'pear']\n",
        "for counter, value in enumerate(my_list):\n",
        "  print(counter, value)\n",
        "'''\n",
        "\n",
        "data = uploaded['corpus'].decode('utf-8')  #Converts bytes into characters.\n",
        "ID = []\n",
        "labels = []\n",
        "texts = []\n",
        "for counter, line in enumerate(data.split('\\n')): #Iterates on every element of the list created by splitting data at \\n\n",
        "  ID.append(counter)\n",
        "  content = line.split() #Split the line to separate label and text\n",
        "  labels.append(content[0])\n",
        "  texts.append(content[1:])\n",
        "texts1 = [\" \".join(line) for line in texts]\n",
        "\n",
        "print(ID[0])\n",
        "print(labels[0])\n",
        "print(texts[0])\n",
        "print(texts1[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "__label__2\n",
            "['Stuning', 'even', 'for', 'the', 'non-gamer:', 'This', 'sound', 'track', 'was', 'beautiful!', 'It', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid.', 'game', 'music!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music!', 'It', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen!', '^_^']\n",
            "Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycTWFljSyf9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "f471e517-6a1f-420f-c047-33fcca3d3de1"
      },
      "source": [
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts1\n",
        "trainDF['label'] = labels\n",
        "\n",
        "print(trainDF.head())\n",
        "print('-------------------------------------------------------------')\n",
        "names = ['Label 1', 'Label_2']\n",
        "print(trainDF[\"label\"].value_counts().plot(kind='pie', labels=names, autopct='%1.0f%%', subplots=True, figsize=(6, 6)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text       label\n",
            "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
            "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
            "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
            "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
            "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
            "-------------------------------------------------------------\n",
            "[<matplotlib.axes._subplots.AxesSubplot object at 0x7f5ca1458320>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKdQav8H7y8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb027c10-bc8e-4fbd-fad3-cf2ed72efd21"
      },
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# Now we label encode our target column so that it can be used in machine learning models\n",
        "# https://www.youtube.com/watch?v=hJ2sKPj5Xn4\n",
        "encoder = LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "\n",
        "print(train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuYrm_TJgvI",
        "colab_type": "text"
      },
      "source": [
        "# 2) Feature Engineering\n",
        "\n",
        "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n",
        "\n",
        "\n",
        "\n",
        "1.   Count Vectors as features\n",
        "2.   TF-IDF Vectors as features\n",
        "\n",
        "*   Word level\n",
        "*   N-Gram level\n",
        "*   Character level\n",
        "\n",
        "3.   Word Embeddings as features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxUHI-hKoYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a count vectorizer object \n",
        "# transform the training and validation data using count vectorizer object\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)\n",
        "\n",
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TneVxjwKvLb",
        "colab_type": "text"
      },
      "source": [
        "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. **Word2Vec** and **GloVe** are the two popular models to create word embedding of a text. These models takes a text corpus as input and produces the word vectors as output. In Bag-of-words model, a text (such as a sentence or a document) is represented as the bag of its words, disregarding grammar. We don't even consider the order of the words in a sentence. We make use of pre-trained word vectors from word2vec and glove in order to do text classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVmM8KkN51Mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a78ceacd-08b8-4b96-af2a-c6984eed2770"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EboQFuyQL8w9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf6b0d70-f621-4e6d-bd13-84cef3f04b44"
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "glove_dir = '/content/gdrive/My Drive/Projects Colab/Glove'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXE2DwZuLcYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYmJBl87P3kj",
        "colab_type": "text"
      },
      "source": [
        "# 3) Model Building\n",
        "\n",
        "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Naive Bayes Classifier\n",
        "\n",
        "*   Linear Classifier\n",
        "\n",
        "*   Support Vector Machine\n",
        "\n",
        "*   Bagging Models\n",
        "\n",
        "*   Boosting Models\n",
        "\n",
        "*   Shallow Neural Networks\n",
        "\n",
        "*   Deep Neural Networks\n",
        "\n",
        "*   Convolutional Neural Network (CNN)\n",
        "\n",
        "*   Long Short Term Model (LSTM)\n",
        "\n",
        "*   Gated Recurrent Unit (GRU)\n",
        "\n",
        "*   Bidirectional RNN\n",
        "\n",
        "*   Recurrent Convolutional Neural Network (RCNN)\n",
        "\n",
        "*   Other Variants of Deep Neural Networks\n",
        "\n",
        "Lets implement these models and understand their details. The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9Xx26Ebu14R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Building\n",
        "# Train a classifier using the features created in the previous step.\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        # predictions = predictions[:,1] This won't work bcoz it's output values are between 0 & 1 rather than strictly 0 & 1. \n",
        "        # argmax returns the position of the largest value. max returns the largest value.\n",
        "        # https://stackoverflow.com/questions/36300334/understanding-argmax\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "        \n",
        "    return metrics.accuracy_score(predictions, valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMFjfEmrJYMh",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHVv-y8pRNRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d0d5175c-ba4f-4b8c-904b-8a130aaadbd9"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print (\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors:  0.8288\n",
            "NB, WordLevel TF-IDF:  0.8388\n",
            "NB, N-Gram Vectors:  0.8408\n",
            "NB, CharLevel Vectors:  0.8104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gM88EcIJWhL",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7o_E8G7w91M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d9541bef-3cce-4384-f515-ff6bd5743232"
      },
      "source": [
        "# Linear Classifier (Logistic Regression)\n",
        "\n",
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print (\"LR, CharLevel Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Glove Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_glove, train_y, xvalid_glove)\n",
        "print (\"LR, Glove Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on word2vec Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_word2vec, train_y, xvalid_word2vec)\n",
        "print (\"LR, word2vec Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.8632\n",
            "LR, WordLevel TF-IDF:  0.866\n",
            "LR, N-Gram Vectors:  0.8292\n",
            "LR, CharLevel Vectors:  0.84\n",
            "LR, Glove Vectors:  0.7368\n",
            "LR, word2vec Vectors:  0.7988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCuZg6YzJPF-",
        "colab_type": "text"
      },
      "source": [
        "#### Support vector machine\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pwnv9AXxjT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9b025667-3bfd-4752-be0a-c55718092373"
      },
      "source": [
        "# Implementing a SVM Model\n",
        "\n",
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# SVM on Glove Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_glove, train_y, xvalid_glove)\n",
        "print (\"SVM, Glove Vectors: \", accuracy)\n",
        "\n",
        "# SVM on word2vec Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_word2vec, train_y, xvalid_word2vec)\n",
        "print (\"SVM, word2vec Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors:  0.5164\n",
            "SVM, Glove Vectors:  0.7292\n",
            "SVM, word2vec Vectors:  0.5164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD6rbnBpJK6S",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmimW-3txji2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9f88a3dc-5223-4e50-a0d7-00c5a7905061"
      },
      "source": [
        "# Bagging Model\n",
        "# Implementing a Random Forest Model\n",
        "\n",
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# RF on Glove Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_glove, train_y, xvalid_glove)\n",
        "print (\"RF, Glove Vectors: \", accuracy)\n",
        "\n",
        "# RF on word2vec Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_word2vec, train_y, xvalid_word2vec)\n",
        "print (\"RF, word2vec Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors:  0.7412\n",
            "RF, WordLevel TF-IDF:  0.7532\n",
            "RF, Glove Vectors:  0.6864\n",
            "RF, word2vec Vectors:  0.7016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW01Oi-bJIRy",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75qo1dgxjnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e0117e5e-4b5b-4a7e-ee96-0a8984c4c1be"
      },
      "source": [
        "# Boosting Model\n",
        "# Implementing Xtereme Gradient Boosting Model\n",
        "\n",
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print (\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print (\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgb, Count Vectors:  0.7924\n",
            "Xgb, WordLevel TF-IDF:  0.7848\n",
            "Xgb, CharLevel Vectors:  0.7948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rChibNZDJCFo",
        "colab_type": "text"
      },
      "source": [
        "#### Shallow Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amquJvjrxjhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "0c149aee-700d-4f89-a180-bdb0fec6956e"
      },
      "source": [
        "def create_model_architecture(input_size):\n",
        "    model = Sequential()\n",
        "\n",
        "    # create input layer and hidden layer\n",
        "    model.add(Dense(100, activation=\"relu\", input_shape=(input_size, )))\n",
        "\n",
        "    # create output layer\n",
        "    # Output layer uses softmax activation.\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "    return model \n",
        "\n",
        "# Output layer should have separate node for each possible outcome.\n",
        "train_y = to_categorical(train_y)\n",
        "#valid_y = to_categorical(valid_y)\n",
        "\n",
        "# NN on Ngram Level TF IDF Vectors\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
        "print (\"NN, Ngram Level TF IDF Vectors:\",  accuracy)\n",
        "\n",
        "# NN on Glove Vectors\n",
        "classifier = create_model_architecture(xtrain_glove.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_glove, train_y, xvalid_glove, is_neural_net=True)\n",
        "print (\"NN, Glove Vectors:\",  accuracy)\n",
        "\n",
        "# NN on word2vec Vectors\n",
        "classifier = create_model_architecture(xtrain_word2vec.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_word2vec, train_y, xvalid_word2vec, is_neural_net=True)\n",
        "print (\"NN, word2vec Vectors:\",  accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 4s 493us/step - loss: 0.4994\n",
            "NN, Ngram Level TF IDF Vectors: 0.8308\n",
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 1s 123us/step - loss: 0.6312\n",
            "NN, Glove Vectors: 0.638\n",
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 1s 129us/step - loss: 0.6019\n",
            "NN, word2vec Vectors: 0.7652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkb0WaO-YrXJ",
        "colab_type": "text"
      },
      "source": [
        "**Recurrent Neural Network – LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZwGA5E2N6f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ca00fd91-9b2b-4b41-a247-686c1fb3bb2f"
      },
      "source": [
        "def create_rnn_lstm():\n",
        "  # Add an Input Layer\n",
        "  input_layer = layers.Input((70, ))\n",
        "\n",
        "  # Add the word embedding Layer\n",
        "  embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "  embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "  # Add the LSTM Layer\n",
        "  lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "  # Add the output Layers\n",
        "  output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "  output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "  output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "  # Compile the model\n",
        "  model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "  model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "  \n",
        "  return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "7500/7500 [==============================] - 27s 4ms/step - loss: 0.6649\n",
            "RNN-LSTM, Word Embeddings 0.5048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzu032W7ZPkz",
        "colab_type": "text"
      },
      "source": [
        "**Recurrent Neural Network – GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nZeZetxYqCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_rnn_gru():\n",
        "  # Add an Input Layer\n",
        "  input_layer = layers.Input((70, ))\n",
        "\n",
        "  # Add the word embedding Layer\n",
        "  embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "  embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "  # Add the GRU Layer\n",
        "  lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "  # Add the output Layers\n",
        "  output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "  output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "  output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "  # Compile the model\n",
        "  model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "  model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "  return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV26ZsJ71rhy",
        "colab_type": "text"
      },
      "source": [
        "**Bidirectional RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDtfSPj21uvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}